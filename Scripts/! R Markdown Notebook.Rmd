---
title: "R Markdown"
author: "Jiri J. Sebek"
date: "2025-03-09"
output:
  pdf_document:
    latex_engine: xelatex
header-includes:
  - \usepackage{fvextra} % Required for advanced verbatim environments
  - \usepackage{fancyvrb} % Enhances verbatim text (code chunks)
  - \usepackage{upquote} % Ensures correct quote formatting in code
  - \usepackage{xcolor} % Required for syntax highlighting
  - \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
  - \RecustomVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Assignment

## Full Data Description

<http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones>

## Assignment Data

<https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip>

## Assignment

1.  You should create one R script called run_analysis.R that does the following.
2.  Merges the training and the test sets to create one data set.
3.  Extracts only the measurements on the mean and standard deviation for each measurement.
4.  Uses descriptive activity names to name the activities in the data set.
5.  Appropriately labels the data set with descriptive variable names.
6.  From the data set in step 4, creates a second, independent tidy data set with the average of each variable for each activity and each subject.

### Download data set, unzip, root directory
```{r}
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip"
download.file(url, destfile = "dataset.zip")
time_stamp <- Sys.time()
unzip("dataset.zip")
formatted_time <- format(time_stamp, "%Y-%m-%d %H:%M:%S")
write(x = formatted_time, file = "The time stamp for dataset download.txt")
```
### Load Libraries
```{r}
# Initialize
# Function to check, install, and load a package
install_and_load <- function(package_name) {
  if (!requireNamespace(package_name, quietly = TRUE)) {
    install.packages(package_name, dependencies = TRUE)
    message(paste("Package", package_name, "installed"))
  }
  suppressMessages(library(package_name, character.only = TRUE))
  message(paste("Package", package_name, "loaded"))
}

# Execution
# install_and_load("readr")
# install_and_load("dplyr")
```
### Group load data to R
```{r}

(function(){
# List all .txt files in the test directory (full path provided)
txt_files <- list.files("UCI HAR Dataset/test", pattern = "\\.txt$", full.names = TRUE)

# Loop through each file
for (file in txt_files) {
  # Extract file name without the directory and extension
  file_name <- tools::file_path_sans_ext(basename(file))
  
  # Read the file; adjust header and other parameters according to your file format
  data <- read.csv(file, header = FALSE)
  
  # Assign the data to an object with the same name as the file (without .txt)
  assign(file_name, data, envir = .GlobalEnv)
}
})()

(function(){
# List all .txt files in the train directory (full path provided)
txt_files <- list.files("UCI HAR Dataset/train", pattern = "\\.txt$", full.names = TRUE)

# Loop through each file
for (file in txt_files) {
  # Extract file name without the directory and extension
  file_name <- tools::file_path_sans_ext(basename(file))
  
  # Read the file; adjust header and other parameters according to your file format
  data <- read.csv(file, header = FALSE)
  
  # Assign the data to an object with the same name as the file (without .txt)
  assign(file_name, data, envir = .GlobalEnv)
}
})()
```

## Loead the Features (Variable Names), Activity Labels
```{r}
features <- read.table(file = "UCI HAR Dataset/features.txt")
activity_labels <- read.table(file = "UCI HAR Dataset/activity_labels.txt")
```

## Parsing the X_test
```{r}
# Parsing each line at every single space
test_x_parsed <- lapply(X_test[1], function(x){
  strsplit(trimws(x), "\\s+")
})

# Bumping up objects by one level (name of column)
test_x_parsed <- test_x_parsed[[1]] 

# Converting to numeric, expecting 1000 original lines of data
test_x_parsed <- lapply(test_x_parsed[1:length(test_x_parsed)], as.numeric)

# Converting to a data frame (expecting 561 columns)
# x <- as.data.frame(test_x_parsed)
# names(x) <- c(1:length(features[,2])) # Features do not seem to be unique
```

## Parsing the X_train
```{r}
# Parsing each line at every single space
train_x_parsed <- lapply(X_train[1], function(x){
  strsplit(trimws(x), "\\s+")
})

# Bumping up objects by one level (name of column)
train_x_parsed <- train_x_parsed[[1]] 

# Converting to numeric, expecting 1000 original lines of data
train_x_parsed <- lapply(train_x_parsed[1:length(train_x_parsed)], as.numeric)

# Converting to a data frame (expecting 561 columns)
# x2 <- as.data.frame(train_x_parsed)
# names(x2) <- c(1:length(features[,2])) # Features do not seem to be unique
```


